---
title: "Beausejour, Jas - MovieLens Simple Script"
author: "Jasmyn Beausejour"
date: "March 18, 2019"
output: html_document
---

```{r, include=FALSE}
start_time <- Sys.time()
```

```{r Creating RMSE Formula}
RMSE <- function(true_ratings, predicted_ratings){
    sqrt(mean((true_ratings - predicted_ratings)^2))
  }
```


```{r Loading libraries, include=FALSE}
if(!require(dslabs)) install.packages("dslabs", repos = "http://cran.us.r-project.org")
library(dslabs)

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
library(tidyverse)

if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")
library(knitr)

if(!require(kableExtra)) install.packages("kableExtra", repos = "http://cran.us.r-project.org")
library(kableExtra)
```

```{r Creating edx and validation, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- read.table(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                      col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data

set.seed(1)
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set

validation <- temp %>% 
     semi_join(edx, by = "movieId") %>%
     semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set

removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)
```

Next, we create our **train_set** and **test_set** data sets.

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
# Test set will be 15% of edX data

set.seed(1)
test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.15, list = FALSE)
train_set <- edx[-test_index,]
temp_set <- edx[test_index,]

# Make sure userId and movieId in test set are also in train set

test_set <- temp_set %>% 
     semi_join(train_set, by = "movieId") %>%
     semi_join(train_set, by = "userId")

# Add rows removed from test set set back into train set

removed_rows <- anti_join(temp_set, test_set)
train_set <- rbind(train_set, removed_rows)

rm(removed_rows, temp_set, test_index)

```

We are now ready to get started.

####Modelling

Our methodology will be very similar to that taught in the course. Slowly but surely, we will build to a model with the following equation.

$$
Y_{u,i} = \mu + \hat{b}_i(\lambda_i) + \hat{b}_u(\lambda_u) + \varepsilon_{u,i}
$$

Here, $Y_{u,i}$ is the actual rating for a given movie, for a given user.  It is the product of $\mu$ the average rating in the dataset, $\hat{b}_i(\lambda_i)$ the regularized movie effect, $\hat{b}_u(\lambda_u)$ the regularized user-specific effect and $\varepsilon_{u,i}$ the residual.

We could also re-write this equation.

$$
Y_{u,i} = \hat{Y_{u,i}}+\varepsilon_{u,i}
$$
This shows that our prediction $\hat{Y_{u,i}}$ plus the residual equals the actual rating. Since the residuals represent the errors of our predictions, we will be striving to minimize $\varepsilon_{u,i}$.

The above is consistent with our RMSE formula:

$$
\mbox{RMSE} = \sqrt{\frac{1}{N} \sum_{u,i}^{} \left( \hat{y}_{u,i} - y_{u,i} \right)^2 } = \sqrt{\frac{1}{N} \sum_{u,i}^{} (\varepsilon_{u,i})^2 }
$$

#####Average Rating

Let's start by making predictions where all predictions are the average rating $\mu$. The model would then be:

$$
Y_{u,i} = \mu +  \varepsilon_{u,i}
$$
```{r}
average_rating <- mean(train_set$rating)

naiveRMSE <- RMSE(test_set$rating, average_rating)

rmse_results <- bind_rows(rmse_results,
                          data_frame(Method = "Just the average", RMSE = naiveRMSE))

kable(rmse_results,align=rep("c",3)) %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```
Of course, here we can see that our RMSE is quite bad.

#####Movie Effect

It is logical to think that, one average, some movies will be rated better, on average, by all users. We would call this the **movie effect**. For each movie, the movie effect is calculated as the average of $Y_{u,i} - \hat{\mu}$ for each movie $i$.

We calculate it using this piece of code:

```{r}
movie_avgs <- train_set %>% 
  group_by(movieId) %>% 
  summarize(movie_effect = mean(rating - average_rating))
```

We can see that these estimates vary substantially:

```{r}
movie_avgs %>% qplot(movie_effect, geom ="histogram", bins = 10, data = ., color = I("black"))
```
  
Since our average rating $\mu$ is about 3.5, a 1.5 movie_effect $b_i$ means a perfect score of 5. 
  
We can now try to estimate each observation of the test set using this movie_effect. To be clear, this is the model we are trying, where $\mu$ is the average rating, $b_i$ is the movie_effect and $\varepsilon_{u,i}$ is the error term:
  
$$
Y_{u,i} = \mu + b_i + \varepsilon_{u,i}
$$
```{r}
predicted_ratings <- average_rating + test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  .$movie_effect

movie_effect_RMSE <- RMSE(predicted_ratings, test_set$rating)

rmse_results <- bind_rows(rmse_results,
                          data_frame(Method= "With Movie Effect",
                                     RMSE=movie_effect_RMSE))

kable(rmse_results,align=rep("c",3), caption = "Metrics calculated on test set only") %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```
This already has provided a nice improvement from `r naiveRMSE` to `r movie_effect_RMSE`. We are still quite far away from our "Netflix Winner" however.

#####Regularizing the Movie Effect

At this point, we make the hypothesis that some of the large movie effects in our data set are due to a very low number of ratings being available for certain movies. For instance, one movie might have been rated only once and given 5 stars. It would therefore have a very high $b_i$ of ~1.5. There is a case to be made that this shouldn't be the case as we have limited information regarding that movie.

First, let's create a database that connects `movieId` to movie title:

```{r}
movie_titles <- edx %>% 
  select(movieId, title) %>%
  distinct()
```

Here are the 10 best movies according to our estimate and how often they were rated:

```{r}
train_set %>% count(movieId) %>% 
  left_join(movie_avgs, by ="movieId") %>%
  left_join(movie_titles, by="movieId") %>%
  arrange(desc(movie_effect)) %>% 
  select(title, movie_effect,n) %>%
  rename("Title"=title,"Movie Effect"=movie_effect,"Times Rated"=n) %>% 
  slice(1:10) %>%  
  kable() %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```

And here are the 10 worst:

```{r}
train_set %>% count(movieId) %>% 
  left_join(movie_avgs, by ="movieId") %>%
  left_join(movie_titles, by="movieId") %>%
  arrange(movie_effect) %>% 
  select(title, movie_effect,n) %>%
  rename("Title"=title,"Movie Effect"=movie_effect,"Times Rated"=n) %>% 
  slice(1:10) %>%  
  kable() %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```

Our hypothesis was right: we hare heavily benefiting or penalizing obscure movies when we actually have very little data points for them. These are noisy estimates that we should not trust, especially when it comes to predictions. Large errors can increase our RMSE, so we would rather be conservative when unsure.

We will now estimate the **regularized_movie_effect** such that:

$$
Regularized\ Movie\ Effect = \hat{b}_i(\lambda) = \frac{1}{\lambda + n_i} \sum_{u=1}^{n_i} \left(Y_{u,i} - \hat{\mu}\right)
$$

where $n_i$ is the number of ratings made for movie $i$.

The intuition here is that as $n_i$ gets bigger, the impact of adding $\lambda$ diminishes. However, for small values of $n_i$, the presence of $\lambda$ reduces the estimate of the movie effect $\hat{b}_i(\lambda)$ .

Let's compute these regularized estimates of **regularized_movie_effect** using 
$\lambda=3$. Later, we will obtimize this term.

```{r}
lambda <- 3

movie_reg_avgs <- train_set %>% 
  group_by(movieId) %>% 
  summarize(reg_movie_effects = sum(rating - average_rating)/(n()+lambda), n_i = n()) 
```

To see how the estimates shrink, let's make a plot of the regularized estimates versus the least squares estimates.

```{r regularization-shrinkage}
data_frame("Original Movie Effect" = movie_avgs$movie_effect, 
           "Regularized Movie Effect" = movie_reg_avgs$reg_movie_effects, 
           n = movie_reg_avgs$n_i) %>%
    ggplot(aes(`Original Movie Effect`, `Regularized Movie Effect`, size=sqrt(n))) + 
        geom_point(shape=1, alpha=0.5)
```

Let's look at the top 10 best movies based on $\hat{b}_i(\lambda)$:

```{r}
train_set %>% count(movieId) %>% 
  left_join(movie_reg_avgs, by ="movieId") %>%
  left_join(movie_titles, by="movieId") %>%
  arrange(desc(reg_movie_effects)) %>% 
  select(title, reg_movie_effects,n) %>%
  rename("Title"=title,"Reg. Movie Effect"=reg_movie_effects,"Times Rated"=n) %>% 
  slice(1:10) %>%  
  kable() %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```

This makes a lot more sense. These movies are widely known as some of the best ever made. 

Let's see if this improves our results further:

```{r}
predicted_ratings <- test_set %>% 
  left_join(movie_reg_avgs, by='movieId') %>%
  mutate(pred = average_rating + reg_movie_effects) %>%
  .$pred

reg_movie_effect_RMSE <- RMSE(predicted_ratings, test_set$rating) 

rmse_results <- bind_rows(rmse_results,
                          data_frame(Method= "With Regularized Movie Effect",
                                     RMSE=reg_movie_effect_RMSE))

kable(rmse_results,align=rep("c",3), caption = "Metrics calculated on test set only") %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)

```

We can see that the improvement from regularization, although quite sensical, is not very large.

This might be due to the fact that we arbitrarily selected $\lambda=3$.

#####Optimizing Lambda for the Movie Effect

Because $\lambda$ is a tuning parameter, we can use cross-validation to choose it.

```{r}
lambdas <- seq(0, 10, 0.25)

mu <- mean(train_set$rating)
just_the_sum <- train_set %>% 
  group_by(movieId) %>% 
  summarize(s = sum(rating - mu), n_i = n())

rmses <- sapply(lambdas, function(l){
  predicted_ratings <- test_set %>% 
    left_join(just_the_sum, by='movieId') %>% 
    mutate(b_i = s/(n_i+l)) %>%
    mutate(pred = mu + b_i) %>%
    .$pred
  return(RMSE(predicted_ratings, test_set$rating))
})
qplot(lambdas, rmses)  
```

Therefore, for the best results, we should use this lambda.

```{r}
lambda_movie <- lambdas[which.min(rmses)]
lambda_movie
```

Let's now use this optimized lambda to see how it improves our results.

```{r}
lambda <- lambda_movie

movie_reg_avgs <- train_set %>% 
  group_by(movieId) %>% 
  summarize(reg_movie_effects = sum(rating - average_rating)/(n()+lambda), n_i = n()) 

predicted_ratings <- test_set %>% 
  left_join(movie_reg_avgs, by='movieId') %>%
  mutate(pred = average_rating + reg_movie_effects) %>%
  .$pred

reg_movie_effect_RMSE <- RMSE(predicted_ratings, test_set$rating) 

rmse_results <- bind_rows(rmse_results,
                          data_frame(Method= "With Optimized Regularized Movie Effect",
                                     RMSE=reg_movie_effect_RMSE))

kable(rmse_results,align=rep("c",3), caption = "Metrics calculated on test set only") %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)

```

There is a slight improvement.

#####User Effect

Of course, we can run the same two previous analysis on the userID. Indeed, some users tend to be much more generous than others as we have seen before.

```{r}
train_set %>% 
  group_by(userId) %>% 
  summarize(b_u = mean(rating)) %>% 
  filter(n()>=100) %>%
  ggplot(aes(b_u)) + 
  geom_histogram(bins = 30, color = "black")+
  labs(title="Count of users by average rating", x="Average Rating by User", y="Count")
```

Here we can calculate the **user_effect** which is a user-specific effect once the **regularized_movie_effect** has been taken into consideration.

To be clear, this is the model we are trying, where $\mu$ is the average rating, $\hat{b}_i(\lambda)$ is the regularized_movie_effect, $b_u$ is the user-specific effect and $\varepsilon_{u,i}$ is the error term:
  
$$
Y_{u,i} = \mu + \hat{b}_i(\lambda) +b_u + \varepsilon_{u,i}
$$

```{r}
user_avgs <- train_set %>% 
  left_join(movie_reg_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(user_effect = mean(rating - average_rating - reg_movie_effects))
```

Let's create predictions see how it improves our model.

```{r}
predicted_ratings <- test_set %>% 
  left_join(movie_reg_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(pred = average_rating + reg_movie_effects + user_effect) %>%
  .$pred

movieanduser_effect_RMSE <- RMSE(predicted_ratings, test_set$rating) 

rmse_results <- bind_rows(rmse_results,
                          data_frame(Method= "With Optimized Regularized Movie and User Effect",
                                     RMSE=movieanduser_effect_RMSE))


kable(rmse_results,align=rep("c",3), caption = "Metrics calculated on test set only") %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```

Quite the improvement! We are now `r paste(round(100*(naiveRMSE-movieanduser_effect_RMSE/naiveRMSE),2),"%",sep="")` better than we were with just the average.

Perhaps we can get even a little bit better by regularizing the user effect and optimizing its lambda.

#####Regularizing the User Effect

For the same reasons we regularized the movie effect, we want to also regularize the user effect. In the code below, we find the best $\lambda_u$ to use for the final model:

$$
Y_{u,i} = \mu + \hat{b}_i(\lambda_i) + \hat{b}_u(\lambda_u) + \varepsilon_{u,i}
$$

```{r best-lambdas}
lambdas <- seq(0, 10, 0.25)

rmses <- sapply(lambdas, function(l){

  mu <- mean(train_set$rating)
  
  reg_b_i <- movie_reg_avgs %>% 
    group_by(movieId) %>%
    summarize(reg_b_i = reg_movie_effects)
  
  reg_b_u <- train_set %>% 
    left_join(reg_b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(reg_b_u = sum(rating - reg_b_i - mu)/(n()+l))

  predicted_ratings <- 
    test_set %>% 
    left_join(reg_b_i, by = "movieId") %>%
    left_join(reg_b_u, by = "userId") %>%
    mutate(pred = mu + reg_b_i + reg_b_u) %>%
    .$pred
  
    return(RMSE(predicted_ratings, test_set$rating))
})

qplot(lambdas, rmses)
lambda_user <- lambdas[which.min(rmses)]
```

Therefore, the best $\lambda_u$ to use would be `r lambda_user` and the RMSE we expect would be `r min(rmses)`; very good indeed! That is only `r paste(round(100*((min(rmses)-best_RMSE)/best_RMSE),2),"%",sep="")` above the Netflix Challenge winners! Obviously, that percent is the hardest to achieve.

Let's update our table.

```{r}
user_reg_avg <- train_set %>% 
    left_join(movie_reg_avgs, by="movieId") %>%
    group_by(userId) %>%
    summarize(reg_user_effects = sum(rating - reg_movie_effects - average_rating)/(n()+lambda_user))

predicted_ratings <- test_set %>% 
  left_join(movie_reg_avgs, by='movieId') %>%
  left_join(user_reg_avg, by='userId') %>%
  mutate(pred = average_rating + reg_movie_effects + reg_user_effects) %>%
  .$pred

final_RMSE <- RMSE(predicted_ratings, test_set$rating) 

rmse_results <- bind_rows(rmse_results,
                          data_frame(Method= "With Optimized Regularized Movie and Optimized Regularized User Effect",
                                     RMSE=final_RMSE))


kable(rmse_results,align=rep("c",3), caption = "Metrics calculated on test set only") %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```

##Results

Let's now visualize where we are with our RMSEs. Let us not forget that so far we have only been using the **edX** dataset. The ultimate test of our algorithm will be on the **validation** set.

```{r}
rmse_results %>% 
  ggplot(aes(y = RMSE, x = reorder(Method,-RMSE)))+
  geom_bar(stat = "identity")+
  coord_flip()+
  labs(x="",y="RMSE")
```

Let's test this model on the **validation** set!

```{r}

average_rating <- mean(edx$rating)

movie_reg_avgs <- edx %>% 
  group_by(movieId) %>% 
  summarize(reg_movie_effects = sum(rating - average_rating)/(n()+lambda_movie), n_i = n()) 

reg_b_i <- movie_reg_avgs %>% 
    group_by(movieId) %>%
    summarize(reg_b_i = reg_movie_effects)
  
reg_b_u <- edx %>% 
    left_join(reg_b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(reg_b_u = sum(rating - reg_b_i - mu)/(n()+lambda_user))

predicted_ratings <- 
    validation %>% 
    left_join(reg_b_i, by = "movieId") %>%
    left_join(reg_b_u, by = "userId") %>%
    mutate(pred = mu + reg_b_i + reg_b_u) %>%
    .$pred
```

Let's verify that we have enough predictions.

- **Rows in Validation**: `r length(validation$userId)`
- **Predictions**: `r length(predicted_ratings)`

Let's now have a look at our final RMSE

```{r}
final_RMSE <- RMSE(validation$rating, predicted_ratings)

rmse_results <- bind_rows(rmse_results,
                          data_frame(Method= "Best Model on Validation Set",
                                     RMSE=final_RMSE))


kable(rmse_results,align=rep("c",3)) %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```

It would seem that, by some luck, our results have improved further. We now boast a RMSE of `r final_RMSE`, which is only `r paste(round(100*((final_RMSE-best_RMSE)/best_RMSE),2),"%",sep="")` higher than the Netflix Challenge winners.

##Conclusion

In conclusion, we have created a linear model that estimates a movie rating with a RMSE of `r final_RMSE`. This is a very good result, especially when compared to the Netflix Challenge winners and their RMSE of `r best_RMSE`. 

We did so by using the following model:

$$
Y_{u,i} = \mu + \hat{b}_i(\lambda_i) + \hat{b}_u(\lambda_u) + \varepsilon_{u,i}
$$
where $\lambda_i=2$ and $\lambda_u=4.75$, which are the only two tuning parameters in our model.

To go further and improve our RMSE, we would recommend using the current predicton and using other machine learning techniques, perhaps on some other data like the genre and the timestamp (or perhaps other data available in the full database) to aimed at minimizing the $\varepsilon_{u,i}$. For the purposes of this report, however, we are satisfied with the final RMSE of `r final_RMSE`, which should award us 25/25 points for this part of the grading.

```{r include=FALSE}
end_time <- Sys.time()
run_time <- end_time - start_time
```
Note: this report took `r round(as.numeric(run_time),2)` minutes to populate.
